{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMRNUKhp6b2li2AetmfS1qk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Felipe-Oliveira11/NLP-Preprocessing/blob/master/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCcPf6buDjpf",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization \n",
        "\n",
        "Dada uma sequência de caracteres e uma unidade de documento definida, a tokenização é a tarefa de dividi-la em pedaços, chamada tokens , talvez ao mesmo tempo descartando certos caracteres, como pontuação.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWiyCCnBG8Y2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "704e63cf-015c-4553-da5e-23ba73e673c6"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install spacy \n",
        "!pip install gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.2.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.37)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.37)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0GWHiIbBLNd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f96c5ffb-32bf-4263-fc99-d0ebec382b68"
      },
      "source": [
        "import os \n",
        "import re\n",
        "import string\n",
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "import gensim\n",
        "from gensim.utils import tokenize\n",
        "from gensim.summarization.textcleaner import split_sentences\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "\n",
        "%matplotlib inline \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjELFvHsKfDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTKym4U-BWJG",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Split Tokenization \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2jXyIE0BPTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da438fff-b003-4815-9d9f-459f1422d389"
      },
      "source": [
        "# Tokenization with split \n",
        "text = 'Today will rain, but I had get my iphone'\n",
        "text.split()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today', 'will', 'rain,', 'but', 'I', 'had', 'get', 'my', 'iphone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1o3YPb4BUMo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81358b2b-5a28-475c-dcf6-556550b5ef09"
      },
      "source": [
        "# Sentence Tokenization \n",
        "text = 'Today will rain, but, I had get my iphone'\n",
        "text.split(',')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today will rain', ' but', ' I had get my iphone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZiCoN0mBks-",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "\n",
        "### Regex \n",
        "\n",
        "\n",
        "Regex são expressões regulares que é basicamente uma sequência de caracteres especial que ajuda a encontrar strings ou conjunto de strings  usando uma sequência como padrão.\n",
        "\n",
        "\n",
        "\n",
        "* A função <b> findall() </b> encontra todas as palavras que correspondem ao padrão transmitido e os armazena na lista. \n",
        "o <b> \\w </b> representa qualquer caractere de palavra que geralmente significa alfanumérico (letras, números) e sublinhado(_) o <b> +</b> significa qualquer número de vezes.\n",
        "\n",
        "\n",
        "* Portanto <b> \"[\\w]+\"</b> diz que o código deve encontrar todos os caracteres alfanuméricos até que qualquer outro caractere seja encontrado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXduh2mvBmiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd3c20d1-0a66-4b07-dc3a-129e4b384054"
      },
      "source": [
        "text = 'Today will rain, but I had get my iphone'\n",
        "\n",
        "tokens = re.findall(\"[\\w]+\", text)\n",
        "tokens"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today', 'will', 'rain', 'but', 'I', 'had', 'get', 'my', 'iphone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdK2zZDIEihm",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "\n",
        "* Função <b>compile()</b> passamos os caracteres que as frases serão divididas assim que um desses caracteres for encontrado. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuiqjPLFCLtk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2785646b-66c8-4971-e541-2c31a0d0c9a3"
      },
      "source": [
        "# Sentence token \n",
        "text = 'Today will rain, but, I had get my iphone '\n",
        "\n",
        "sentences = re.compile('[,!?] ').split(text)\n",
        "sentences"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today will rain', 'but', 'I had get my iphone ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfWXsgZcGyR2",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "### NLTK \n",
        "\n",
        "Natural Language Toolkit, contém um módulo chamado <b> tokenize() </b> que é classificado em duas subcategorias: \n",
        "\n",
        "* Word tokenize: Usado para dividir a frase em tokens ou palavras <b> word_tokenize() </b>\n",
        "\n",
        "* Sentence tokenize: Usado para dividir um documento ou parágrafo em frases <b> sent_tokenize() </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4TKkCnzIMUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "b00dff00-83a4-497d-9283-850cf4a8efab"
      },
      "source": [
        "# word tokenize \n",
        "text = 'Founded in 2002, SpaceX mission is to enable humans to become a spacefaring civilization'\n",
        "word_tokenize(text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " '2002',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqjpI_NLIqiI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4053d2e1-4c04-4d40-b6f6-938cf80638de"
      },
      "source": [
        "# tamanho do token \n",
        "len(word_tokenize(text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H5qpc7kJCj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "723edf93-2729-46b9-da96-de73ca835a82"
      },
      "source": [
        "# sentence tokenize \n",
        "text = \"\"\"Founded in 2002, SpaceX mission is to enable humans to become a spacefaring civilization. tesla is\n",
        "other great company at san francisco bay area\"\"\"\n",
        "sent_tokenize(text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded in 2002, SpaceX mission is to enable humans to become a spacefaring civilization.',\n",
              " 'tesla is\\nother great company at san francisco bay area']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqmKqyBWJhTm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9b7e3db-18d5-43dc-e3bd-4a8bece13421"
      },
      "source": [
        "len(sent_tokenize(text))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T0nSraaKWXu",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "\n",
        "### Spacy \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyg9phsZKY8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# carregar o Tokenizador em inglês \n",
        "nlp = English()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga5AG2VULRHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"Founded in 2002, SpaceX mission is to enable humans to become a spacefaring civilization. tesla is\n",
        "other great company at san francisco bay area\"\"\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvp3ffxtK4N2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "f5bf07fd-5303-41d1-f9c8-79e9c810283f"
      },
      "source": [
        "# objeto nlp é usado para criar um documento com anotações linguisticas \n",
        "\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# criando lista de tokens de palavras \n",
        "\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "  token_list.append(token.text)\n",
        "\n",
        "\n",
        "token_list"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " '2002',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " '.',\n",
              " 'tesla',\n",
              " 'is',\n",
              " '\\n',\n",
              " 'other',\n",
              " 'great',\n",
              " 'company',\n",
              " 'at',\n",
              " 'san',\n",
              " 'francisco',\n",
              " 'bay',\n",
              " 'area']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfVUS23lL5PB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af64c82a-5624-49f0-a566-f6b34512d6b6"
      },
      "source": [
        "# sentence tokenization Spacy \n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# criando pipeline \n",
        "sent = nlp.create_pipe('sentencizer')\n",
        "\n",
        "# adicionando componente no pipeline\n",
        "nlp.add_pipe(sent)\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "sent_list = []\n",
        "\n",
        "for sentence in doc.sents:\n",
        "  sent_list.append(sentence.text)\n",
        "\n",
        "sent_list"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded in 2002, SpaceX mission is to enable humans to become a spacefaring civilization.',\n",
              " 'tesla is\\nother great company at san francisco bay area']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Io-Xo28O4D9",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "\n",
        "### TensorFlow e Keras \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6dpiRDPO8U2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"Founded in 2002, SpaceX mission is to enable humans to become a spacefaring civilization. tesla is\n",
        "other great company at san francisco bay area\"\"\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLD89pAkPBX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "12352719-8041-46c3-9c01-f9ed573d2a52"
      },
      "source": [
        "token = text_to_word_sequence(text)\n",
        "token"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['founded',\n",
              " 'in',\n",
              " '2002',\n",
              " 'spacex',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'tesla',\n",
              " 'is',\n",
              " 'other',\n",
              " 'great',\n",
              " 'company',\n",
              " 'at',\n",
              " 'san',\n",
              " 'francisco',\n",
              " 'bay',\n",
              " 'area']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EfA7O_4acIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtHPrxdaghi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentença (frase)\n",
        "sentences = [\n",
        "           'I love my dog',\n",
        "           'I love my cat',\n",
        "           'I love my Horse'\n",
        "           ]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URZmp69Aag1Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "425c3875-7d4a-4e56-fd4c-b5549d95be7b"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5, 'horse': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy65QTNnPNHt",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "\n",
        "### Gensim \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuW1fg4OPPS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"Founded in 2002, SpaceX mission is to enable humans to become a spacefaring civilization. tesla is\n",
        "other great company at san francisco bay area\"\"\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AksqDCfqPdf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "fea998ce-c852-4d21-9958-1081e546fa76"
      },
      "source": [
        "# word tokenizer \n",
        "token = list(tokenize(text))\n",
        "token"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " 'SpaceX',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'tesla',\n",
              " 'is',\n",
              " 'other',\n",
              " 'great',\n",
              " 'company',\n",
              " 'at',\n",
              " 'san',\n",
              " 'francisco',\n",
              " 'bay',\n",
              " 'area']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmjA1ROBPjfs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "898dea96-aac8-4a62-fd05-baf305290799"
      },
      "source": [
        "# sentence tokenizer \n",
        "sentence_token = split_sentences(text)\n",
        "sentence_token"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded in 2002, SpaceX mission is to enable humans to become a spacefaring civilization.',\n",
              " 'tesla is',\n",
              " 'other great company at san francisco bay area']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}